{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp-BVPuZAtqk"
   },
   "source": [
    "# Matthew Sinclair - HeartLab Take-Home Test Submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8DAu9PcA2jH"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 389,
     "status": "ok",
     "timestamp": 1631832288735,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "2F2Wsk6Dc6pM"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras.losses\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import adam_v2\n",
    "\n",
    "from pathlib import Path\n",
    "from enum import Enum\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkBiUgEUBH-U"
   },
   "source": [
    "## Global Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SrImxslDBX1_"
   },
   "source": [
    "### Path Specifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1631832291350,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "kOgI82yYrSaY"
   },
   "outputs": [],
   "source": [
    "# Specify the path to the project folder in google drive. \n",
    "# PROJECT_PATH = \"E:/MattDataHelp/chest_xray\"\n",
    "\n",
    "# Change the current working directory to the project folder.\n",
    "# os.chdir(PROJECT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1631832291351,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "dZy6H2Z-kzBl"
   },
   "outputs": [],
   "source": [
    "# Specify paths to project data directories.\n",
    "CHEST_XRAY_PATH =  \"E:/MattDataHelp/chest_xray/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1631832291847,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "H-C6IvN0s8j8"
   },
   "outputs": [],
   "source": [
    "# # Specify path to the requirements.txt file\n",
    "# REQUIREMENTS_PATH = Path(PROJECT_PATH, 'requirements.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7q5KW22YBcy8"
   },
   "source": [
    "### Target Class Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1631832292893,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "Td5N24KD1Rtv"
   },
   "outputs": [],
   "source": [
    "# Create an enumeration for human-readable Targets\n",
    "class Target(Enum):\n",
    "  NORMAL = 0\n",
    "  BACTERIA = 1\n",
    "  VIRUS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sShxJO4VBnmZ"
   },
   "source": [
    "### Image Resize Dimensions\n",
    "Specify the dimensions imported images should be resized to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1631832293248,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "lnoVqv-FByZH"
   },
   "outputs": [],
   "source": [
    "# Modified the image size to fit the required dimensions for DenseNet\n",
    "IMAGE_SIZE = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMSSf7YPCVcZ"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u4PXKUdcCfyp"
   },
   "source": [
    "### img_2_arr\n",
    "Reads in an image with a specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1631832294597,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "wsrumCcnc6pS"
   },
   "outputs": [],
   "source": [
    "def img_2_arr(\n",
    "    img_path: str,\n",
    "    resize: bool = False,\n",
    "    grayscale: bool = True,\n",
    "    size: tuple = (256, 256),\n",
    ") -> np.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "    This function is responsible for opening an image, Preprocessing\n",
    "    it by color or size and returning a numpy array.\n",
    "\n",
    "    Input:\n",
    "        - img_path: str, a path to the location of a image file on disk\n",
    "        - resize: bool, True/False if the image is to be resized\n",
    "        - grayscale: bool, True/False if image is meant to be B&W or color\n",
    "        - size: tuple, a 2d tuple containing the x/y size of the image.\n",
    "\n",
    "    Output:\n",
    "        - a np.ndarray which is assosiated to the image that was input.\n",
    "    \"\"\"\n",
    "\n",
    "    if grayscale:\n",
    "        img_arr = cv2.imread(img_path, 0)\n",
    "    else:\n",
    "        img_arr = cv2.imread(img_path)\n",
    "\n",
    "    if resize:\n",
    "        img_arr = cv2.resize(img_arr, size)\n",
    "\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UMIK8etC4kx"
   },
   "source": [
    "### create_datasets\n",
    "Adapted slightly from the provided function (to make use of target enums) this function reads in images from all subdirectories of the supplied directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1631832295545,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "H8PLIOBkc6pT"
   },
   "outputs": [],
   "source": [
    "def create_datasets(data_dir: str) -> np.ndarray:\n",
    "  \"\"\"\n",
    "  This function is responsible for creating a dataset which\n",
    "  contains all images and their associated class.\n",
    "\n",
    "  Inputs:\n",
    "      - data_dir: str, which is the location where the chest x-rays are\n",
    "          located.\n",
    "\n",
    "  Outputs:\n",
    "      - a np.ndarray which contains the processed image, and the class\n",
    "          int, associated with that class.\n",
    "\n",
    "  \"\"\"\n",
    "  # Create lists to store all image pathways\n",
    "  all_normal_img_paths = []\n",
    "  all_viral_img_paths = []\n",
    "  all_bact_img_paths = []\n",
    "\n",
    "  print(\"Getting Paths...\")\n",
    "  for cls in os.listdir(data_dir): # NORMAL or PNEUMONIA\n",
    "    print(\" Class:\", cls)\n",
    "    for img in os.listdir(os.path.join(data_dir, cls)): # all images\n",
    "      if cls == \"NORMAL\":\n",
    "          all_normal_img_paths.append(os.path.join(data_dir, cls, img))\n",
    "      elif \"virus\" in img:\n",
    "          all_viral_img_paths.append(os.path.join(data_dir, cls, img))\n",
    "      else:\n",
    "          all_bact_img_paths.append(os.path.join(data_dir, cls, img))\n",
    "\n",
    "  print(\"Reading Images...\")\n",
    "\n",
    "  # Use Global Target Enum\n",
    "  dataset = (\n",
    "      [\n",
    "          [img_2_arr(path, grayscale=True, resize=True, size=IMAGE_SIZE), Target.NORMAL.value]\n",
    "          for path in all_normal_img_paths\n",
    "      ]\n",
    "      + [\n",
    "          [img_2_arr(path, grayscale=True, resize=True, size=IMAGE_SIZE), Target.BACTERIA.value]\n",
    "          for path in all_bact_img_paths\n",
    "      ]\n",
    "      + [\n",
    "          [img_2_arr(path, grayscale=True, resize=True, size=IMAGE_SIZE), Target.VIRUS.value]\n",
    "          for path in all_viral_img_paths\n",
    "      ]\n",
    "  )\n",
    "\n",
    "  print(\"Dataset Created!\")\n",
    "\n",
    "  return np.array(dataset, dtype=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ShP3RVgDS8k"
   },
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12997,
     "status": "ok",
     "timestamp": 1631832309304,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "PBtEDbfeu3dI",
    "outputId": "9c83aeb0-768d-47e2-a4e6-6479f04e430a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting Paths...\n",
      " Class: NORMAL\n",
      " Class: PNEUMONIA\n",
      "Reading Images...\n",
      "Dataset Created!\n"
     ]
    }
   ],
   "source": [
    "data_dir_str = r'{}'.format(str(CHEST_XRAY_PATH))\n",
    "# get dataset\n",
    "dataset = create_datasets(data_dir_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JodyJyjnDe6v"
   },
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c63milaxDoCr"
   },
   "source": [
    "### Target Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "ok",
     "timestamp": 1631832309305,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "3hOpHyDF2sWg",
    "outputId": "6348ad50-b478-4fe2-98c5-2888373e2bb3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5216,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the class labels / target values from the dataset array\n",
    "targets = dataset[:,1].astype(np.int32)\n",
    "# Check this is a vector\n",
    "targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1631832309307,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "ydC3AeD38Kag",
    "outputId": "359c140f-4b0b-4125-90fa-e8cb9783b118"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAE/CAYAAADohqLkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAATmUlEQVR4nO3df6zdd33f8ecrCVRtyRazeFlwTJ0ht8y0q0mtkA3QUtHmF2wJTKO2aAkolamUqA0q3QJsSlQULWxNqWghUzqsJhNNGo2meK234EWpAlqBOMyEOIHFg2SxFxKHUKCDMSV974/zuXC4vje+9r1+32v7+ZCu7jmf74/zOTkn93nP93zvcaoKSZLU46TlnoAkSScSwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSo1OWewLP5/TTT69169Yt9zQkSTos999//9NVtXquZSs6vOvWrWPXrl3LPQ1Jkg5LksfmW+ahZkmSGhleSZIaGV5JkhoZXkmSGhleSZIaGV5JkhoZXkmSGhleSZIaGV5JkhoZXkmSGhleSZIaHTK8SdYmuSfJQ0n2JPm1MX5dkv1Jdo+vS6a2eXeSvUm+lOTCqfGLxtjeJNccnbskSdLKtZB/JOFZ4Ner6nNJTgXuT7JzLPtAVf3W9MpJNgCbgVcALwH+a5IfH4s/BPw8sA+4L8n2qnpoKe6ItNTWXfNnyz0FLdKjN7x+uacgHeSQ4a2qJ4AnxuVvJXkYWPM8m1wK3F5V3wW+kmQvcO5YtreqvgyQ5PaxruGVJJ0wDus93iTrgFcCnxlDVyV5IMm2JKvG2Brg8anN9o2x+cYlSTphLDi8SV4EfAy4uqq+CdwEvAzYyOQV8Y1LMaEkW5PsSrLrwIEDS7FLSZJWjAWFN8kLmET3o1X1xwBV9WRVPVdVfw38Pt8/nLwfWDu1+VljbL7xH1BVN1fVpqratHr16sO9P5IkrWgLOas5wEeAh6vqt6fGz5xa7Y3Ag+PydmBzkh9KcjawHvgscB+wPsnZSV7I5ASs7UtzNyRJOjYs5KzmVwO/BHwhye4x9h5gS5KNQAGPAu8AqKo9Se5gctLUs8CVVfUcQJKrgLuAk4FtVbVnye6JJEnHgIWc1fwpIHMs2vE821wPXD/H+I7n206SpOOdn1wlSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSo0OGN8naJPckeSjJniS/NsZfnGRnkkfG91VjPEk+mGRvkgeSnDO1r8vH+o8kufzo3S1JklamhbzifRb49araAJwHXJlkA3ANcHdVrQfuHtcBLgbWj6+twE0wCTVwLfAq4Fzg2plYS5J0ojhkeKvqiar63Lj8LeBhYA1wKXDLWO0W4LJx+VLg1pr4NHBakjOBC4GdVfVMVX0d2AlctJR3RpKkle6w3uNNsg54JfAZ4IyqemIs+ipwxri8Bnh8arN9Y2y+cUmSThgLDm+SFwEfA66uqm9OL6uqAmopJpRka5JdSXYdOHBgKXYpSdKKsaDwJnkBk+h+tKr+eAw/OQ4hM74/Ncb3A2unNj9rjM03/gOq6uaq2lRVm1avXn0490WSpBVvIWc1B/gI8HBV/fbUou3AzJnJlwMfnxp/6zi7+TzgG+OQ9F3ABUlWjZOqLhhjkiSdME5ZwDqvBn4J+EKS3WPsPcANwB1JrgAeA948lu0ALgH2At8G3g5QVc8keR9w31jvN6vqmaW4E5IkHSsOGd6q+hSQeRa/bo71C7hynn1tA7YdzgQlSTqe+MlVkiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1MrySJDUyvJIkNTK8kiQ1OmR4k2xL8lSSB6fGrkuyP8nu8XXJ1LJ3J9mb5EtJLpwav2iM7U1yzdLfFUmSVr6FvOL9A+CiOcY/UFUbx9cOgCQbgM3AK8Y2H05ycpKTgQ8BFwMbgC1jXUmSTiinHGqFqro3yboF7u9S4Paq+i7wlSR7gXPHsr1V9WWAJLePdR86/ClLknTsWsx7vFcleWAcil41xtYAj0+ts2+MzTd+kCRbk+xKsuvAgQOLmJ4kSSvPkYb3JuBlwEbgCeDGpZpQVd1cVZuqatPq1auXareSJK0IhzzUPJeqenLmcpLfB/50XN0PrJ1a9awxxvOMS5J0wjiiV7xJzpy6+kZg5ozn7cDmJD+U5GxgPfBZ4D5gfZKzk7yQyQlY24982pIkHZsO+Yo3yW3A+cDpSfYB1wLnJ9kIFPAo8A6AqtqT5A4mJ009C1xZVc+N/VwF3AWcDGyrqj1LfWckSVrpFnJW85Y5hj/yPOtfD1w/x/gOYMdhzU6SpOOMn1wlSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUqMj+mcBj1Xrrvmz5Z6CFuHRG16/3FOQnpc/Y45tXT9jfMUrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSo0OGN8m2JE8leXBq7MVJdiZ5ZHxfNcaT5INJ9iZ5IMk5U9tcPtZ/JMnlR+fuSJK0si3kFe8fABfNGrsGuLuq1gN3j+sAFwPrx9dW4CaYhBq4FngVcC5w7UysJUk6kRwyvFV1L/DMrOFLgVvG5VuAy6bGb62JTwOnJTkTuBDYWVXPVNXXgZ0cHHNJko57R/oe7xlV9cS4/FXgjHF5DfD41Hr7xth84wdJsjXJriS7Dhw4cITTkyRpZVr0yVVVVUAtwVxm9ndzVW2qqk2rV69eqt1KkrQiHGl4nxyHkBnfnxrj+4G1U+udNcbmG5ck6YRypOHdDsycmXw58PGp8beOs5vPA74xDknfBVyQZNU4qeqCMSZJ0gnllEOtkOQ24Hzg9CT7mJydfANwR5IrgMeAN4/VdwCXAHuBbwNvB6iqZ5K8D7hvrPebVTX7hC1Jko57hwxvVW2ZZ9Hr5li3gCvn2c82YNthzU6SpOOMn1wlSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSI8MrSVIjwytJUiPDK0lSo0WFN8mjSb6QZHeSXWPsxUl2JnlkfF81xpPkg0n2JnkgyTlLcQckSTqWLMUr3p+tqo1VtWlcvwa4u6rWA3eP6wAXA+vH11bgpiW4bUmSjilH41DzpcAt4/ItwGVT47fWxKeB05KceRRuX5KkFWux4S3gE0nuT7J1jJ1RVU+My18FzhiX1wCPT227b4xJknTCOGWR27+mqvYn+dvAziRfnF5YVZWkDmeHI+BbAV760pcucnqSJK0si3rFW1X7x/engDuBc4EnZw4hj+9PjdX3A2unNj9rjM3e581VtamqNq1evXox05MkacU54vAm+dEkp85cBi4AHgS2A5eP1S4HPj4ubwfeOs5uPg/4xtQhaUmSTgiLOdR8BnBnkpn9/GFV/Zck9wF3JLkCeAx481h/B3AJsBf4NvD2Rdy2JEnHpCMOb1V9GfjpOca/BrxujvECrjzS25Mk6XjgJ1dJktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUyPBKktTI8EqS1MjwSpLUqD28SS5K8qUke5Nc0337kiQtp9bwJjkZ+BBwMbAB2JJkQ+ccJElaTt2veM8F9lbVl6vq/wG3A5c2z0GSpGXTHd41wONT1/eNMUmSTginLPcEZkuyFdg6rv5Vki8t53yOMacDTy/3JI6WvH+5Z3DcOa6fL+Bz5ig4rp8zS/x8+bH5FnSHdz+wdur6WWPse6rqZuDmzkkdL5LsqqpNyz0PHRt8vuhw+ZxZGt2Hmu8D1ic5O8kLgc3A9uY5SJK0bFpf8VbVs0muAu4CTga2VdWezjlIkrSc2t/jraodwI7u2z1BeIheh8Pniw6Xz5klkKpa7jlIknTC8CMjJUlqZHiXUZJKcuPU9XcluW7q+tYkXxxfn03ymqllfz4+evPzSe5LsnFq2aNJPjnrtnYneXDW2O8k2Z/kpKmxtyX5vaW9pzpSSZ4bj93nk3wuyT+ctfzqJP83yd+cNX5xkl1JHkry35PcmOS9Y1+7p/a7O8mvJrluPBd2T32dluT8JN8Y17+Y5LembuOg58pY7/aj+19Fi5HkniQXzhq7Osl/nvkZcYjH/bok75q1/aNJTh+X35tkT5IHxvav6rhfxxLDu7y+C7xp5gk7LckbgHcAr6mqlwO/Avxhkr8ztdpbquqngQ8D/3bWLk5Nsnbs6+/Nsf+TgDcy+UCTf7QUd0ZHxXeqauN4nN8N/OtZy7cw+WuBN80MJPlJ4PeAX6yqDcAmJp8Yd/3Y18ap/W6sqg+OTT8wNbaxqv5yjH9ybPNK4A1JXj3XRMfz7GTgtUl+dAnuu46O25j8Rcm0zRz83FrQ4z4tyT8A3gCcU1V/H/g5fvBDk4ThXW7PMjlZ4Z1zLPsXwG9U1dMAVfU54BbgyjnW/QsO/gSwO4BfGJe3MPmfbdr5wB7gprFcK9/fAL4+cyXJy4AXAf+SH3wM/zlwfVV9EaCqnquqmxZ741X1HWA383/a3BbgPwCfwI+CXcn+I/D68SedJFkHvIR5ArmAx33amcDTVfXdse3TVfW/l2DOxxXDu/w+BLxl9qFC4BXA/bPGdo3x2S4C/mTW2Mf4/qugfwz8p1nLZ2J8J5P/CV9weNNWkx+eOdwH/HvgfVPLNjP5vPNPAj+R5Iwx/pMc/NxZiHdOHWa+Z/bCJKuA9cC982z/C2M+t+EvcytWVT0DfJbJP1YDk+fRHcCcZ9ou4HGf9glgbZL/keTDSTyaNgfDu8yq6pvArcCvHsHmH03yFeC9TAI+7WvA15NsBh4Gvj2zYPymewnwJ+P2PwNciFaimUPCL2fyC9atSTKWbQFur6q/ZvKL1j9b5G1NH2r+2anx1yb5PJNPmburqr46e8Mkm5i80vlfwN3AK5O8eJHz0dEzfbh5MwcfEYP5H/f5/hSmquqvgJ9h8rG/B4A/SvK2JZv1ccLwrgy/A1wBTL8v9hCTJ/C0n2FyeHjGW4C/y+QQ9O/Osd8/YhLk2f9TXQicBnwhyaPAa/AVyopXVX/B5LNyVyf5KSavQnaOx3Az338M93Dwc2cxPjneY34FcMX0iXxTtgAvH3P5n0wOi//TJZyDltbHgdclOQf4kaqa6wjJfI/714BVs9Y9FfhL+N5bG39eVdcCV+Hz4CCGdwUYh37uYBLfGf8GeH+SvwUwnvRvY3Ii1fS2Bfwr4LwkL5+16zvHfu6aNb4F+OWqWldV64CzgZ9P8iNLcX90dIzH92QmP/i2ANfNPIZV9RLgJUl+jMmJdu9J8uNju5OS/Mpib7+qvgLcwOT8g+l5nQS8GfipqefUpfjL3Io1XpneA2xj7le70+vOftzvBf5JklMBkrwJ+HxVPZfkJ5Ksn9p8I/DYEk//mLfi/nWiE9iNTH47BKCqtidZA/y3JAV8i8lZqk/M3rCqvpPJnyX9BlPxrqpvAe8HmDk6OeJ6EZOzpGfW+z9JPsXkvWCAtyW5bOomzquqfUtxJ3XYfjjJ7nE5wOXjB9xmJm8XTLsT2FxV709yNXDbeLwL+NMF3NY7k/zi1PXL5ljn3wHvGifkzHgtsH/WSTT3AhuSnDnXc1Yrwsw5HrPPcJ7L9x73qnpg/BnZp8bPpqeAXx7rvQj43SSnMTl5dC/f/9fmNPjJVZIkNfJQsyRJjQyvJEmNDK8kSY0MryRJjQyvJEmNDK8kSY0MryRJjQyvJEmN/j+iXrwrUC5DxwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the class labels as a bar chart to examine class distribution\n",
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "target_names = [target.name for target in Target]\n",
    "counts = np.bincount(targets)\n",
    "ax.bar(target_names,counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4Krqrf_2sJi"
   },
   "source": [
    "It looks like the dataset is slightly imbalanced, with many more instances of the bacterial pneumonia class than the other two classes. \n",
    "\n",
    "We should use stratified cross-fold validation to ensure accurate model evaluation. \n",
    "\n",
    "After fitting the model we can determine if upsampling / downsampling / weighted loss is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXTNDrnfDsTK"
   },
   "source": [
    "### Image Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1631832309308,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "aLw57LbF2rgd",
    "outputId": "a3af7e7b-e99b-40c8-8430-e31c2b3896e4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's confirm the dimensions of the images\n",
    "images = dataset[:,0]\n",
    "image_shapes = np.array([image.shape for image in images])\n",
    "\n",
    "np.all(image_shapes == np.array(IMAGE_SIZE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrGyKeWN_qbE"
   },
   "source": [
    "As expected, all images have shape (224, 224). This is required for the DenseNet architecture. As they are Greyscale they have no color channel, hence they have only 2 dimensions, not 3.\n",
    "\n",
    "However, this is achieved by resizing which will result in the images being 'squashed' to fit. To my knowledge, this is fine for a neural network (perhaps even good as it prevents overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tJAraFiHI8jD"
   },
   "source": [
    "### Other thoughts on the data / potential hidden bias\n",
    "\n",
    "* When fitting any model (neural net or otherwise) it is really important to ensure that the sample population reflects of the population of interest in model deployment. That is, it might not make sense to fit a model to people from Japan, and expect it to definitely work for all people around the world. I don't know what variation there is between pneumonia scans - this is a question for the medical expert. \n",
    "\n",
    "understand that the predictive value may not generalise to the I don't have any demographic information about each scan in the dataset, such as age, gender, fitness level, geographic location, and so on. It could be that one or more of these characteristics confound the entire model fitting process. Neural nets are basically overfitting machines - if there is anything that reliably enables the model to classify the pneumonia type for this dataset alone, it will definitely use it. This would then report a artificially high training and testing accuracy.\n",
    "\n",
    "* I don't know whether the same individual was scanned both 'having pneumonia' and 'not having pneumonia'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4j7RtT2uAkyc"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1631835335921,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "yIXMpKK7c6pX"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten\n",
    "\n",
    "def model_zero(loss='categorical_crossentropy', opt='adam'):\n",
    "    \"\"\"\n",
    "    This function implements a small example model that you can use/modify.\n",
    "    You are not limited to creating your own model, you can use already\n",
    "    implemented models such as ResNets etc.\n",
    "    \"\"\"\n",
    "    # building a linear stack of layers with the sequential model\n",
    "    model = Sequential()\n",
    "    # convolutional layer\n",
    "    model.add(Conv2D(25, kernel_size=(3,3), strides=(1,1), padding='valid', activation='relu', input_shape=(28,28,1)))\n",
    "    model.add(MaxPool2D(pool_size=(1,1)))\n",
    "    # flatten output of conv\n",
    "    model.add(Flatten())\n",
    "    # hidden layer\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    # output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # compiling the sequential model\n",
    "    model.compile(loss=loss, metrics=['accuracy'], optimizer=opt)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 3005,
     "status": "ok",
     "timestamp": 1631835414614,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "gclem4nWQ14b"
   },
   "outputs": [],
   "source": [
    "DenseNet = tf.keras.models.Sequential([\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # # The fifth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('dandelions') and 1 for the other ('grass')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1631835414616,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "YG8H-BRUc6tw"
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset, stratify=targets, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1631835414619,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "E4JHwoqadZ7O"
   },
   "outputs": [],
   "source": [
    "X_train, y_train = train[:,0], train[:,1]\n",
    "X_test, y_test = test[:,0], test[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1631835414621,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "8aHiaWCPd-2U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "\n",
    "optimizer = adam_v2.Adam(learning_rate=LEARNING_RATE)\n",
    "print(\"Learning rate:\", LEARNING_RATE)\n",
    "\n",
    "# sparse as Y values are stored as integers (0-42), rather than one-hot encoded (i.e. 3 class, [1,0,0], [0,1,0], [0,0,1])\n",
    "loss = tensorflow.keras.losses.categorical_crossentropy\n",
    "\n",
    "DenseNet.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 37,
     "status": "ok",
     "timestamp": 1631835414624,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "4wrVQTkNtvB5"
   },
   "outputs": [],
   "source": [
    "def img_arr_2_4Dtensor(img_arr):\n",
    "  return tf.convert_to_tensor([tf.convert_to_tensor(np.expand_dims(img, axis = img.ndim)) for img in img_arr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1631835414626,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "dpJjfYaD6LM9"
   },
   "outputs": [],
   "source": [
    "def label_arr_2_binary(label_arr):\n",
    "  return tf.convert_to_tensor([[x == 0, x==1, x==2] for x in label_arr], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1631835415111,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "md3KkOKni09K"
   },
   "outputs": [],
   "source": [
    "X_train = img_arr_2_4Dtensor(X_train)\n",
    "X_train = tf.image.grayscale_to_rgb(X_train)\n",
    "y_train = label_arr_2_binary(y_train)\n",
    "\n",
    "\n",
    "X_test = img_arr_2_4Dtensor(X_test)\n",
    "X_test = tf.image.grayscale_to_rgb(X_test)\n",
    "y_test = label_arr_2_binary(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 563432,
     "status": "ok",
     "timestamp": 1631835979329,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "JoNh0y0NfFew"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py:788 train_step\n        loss = self.compiled_loss(\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\losses.py:1665 categorical_crossentropy\n        return backend.categorical_crossentropy(\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\backend.py:4839 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (32, 3) and (32, 1) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27572/1133089064.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDenseNet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1182\u001b[0m                 _r=1):\n\u001b[0;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    757\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 759\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    760\u001b[0m             *args, **kwds))\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3065\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3066\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3067\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3463\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3296\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3297\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3298\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3299\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    666\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    667\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 668\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    669\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    992\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 994\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    995\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py:853 train_function  *\n        return step_function(self, iterator)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py:835 run_step  **\n        outputs = model.train_step(data)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\training.py:788 train_step\n        loss = self.compiled_loss(\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\engine\\compile_utils.py:201 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\losses.py:141 __call__\n        losses = call_fn(y_true, y_pred)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\losses.py:245 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\losses.py:1665 categorical_crossentropy\n        return backend.categorical_crossentropy(\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\keras\\backend.py:4839 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    e:\\compsci 760 - animal indentification\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py:1161 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (32, 3) and (32, 1) are incompatible\n"
     ]
    }
   ],
   "source": [
    "history = DenseNet.fit(x=X_train, y=y_train, validation_split = 0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5069,
     "status": "ok",
     "timestamp": 1631835986808,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "sDvzFMfS4GoR"
   },
   "outputs": [],
   "source": [
    "model_predictions = DenseNet.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.math.argmax(model_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DenseNet.evaluate(x = X_test, y = y_test, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 494,
     "status": "ok",
     "timestamp": 1631836014874,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "oCfl6bMM7fD7"
   },
   "outputs": [],
   "source": [
    "argmax_predictions = tf.math.argmax(predictions_softmax, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1631836014878,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "CABNJJva7mTF"
   },
   "outputs": [],
   "source": [
    "y_test_label = tf.math.argmax(y_test, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_t3LgweFAjUL"
   },
   "source": [
    "## Sources\n",
    "\n",
    "- [Medium Article](https://www.jeremyjordan.me/convnet-architectures/#densenet)\n",
    "- [DenseNet Paper](https://arxiv.org/pdf/1608.06993v3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKRewYC_EcYZ"
   },
   "source": [
    "### Possible Extensions\n",
    "\n",
    "As I was completing this exercise, I thought  of a few things that might further improve accuracy. However, I think they are beyond the scope of what was being asked for in this task. I would be happy to implement any of them if you'd like, however :)\n",
    "\n",
    "* Elastic deformations. Slightly stretching / deforming each image would mean we have many more training exsamples. This would be particularly useful for the minority classes for upsampling.\n",
    "* Random Rotations. These would likely only be useful within a certain range, as I imagine X-rays are usually presented in orientation provided. There is no point training the model to make the classification on images that are upside down, for example. However, small rotations (maybe <= 45 degrees in each direction) might still be useful.\n",
    "* Vertical reflection transformation (This might not make sense either though - I know that lungs are anatomically asymmetrical to accommodate the structures of the heart.\n",
    "* Cropping\n",
    "*  Add noise to image (normal integer values added to each image)\n",
    "*  Occlusions (Show only one lung / part of a lung on some training images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h20ilfdWGBCZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 23104,
     "status": "error",
     "timestamp": 1631764304606,
     "user": {
      "displayName": "Matthew Sinclair",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhYDj5d6lXGU6L-PujSykKrjDagMfBKNa3zRTQJ1Q=s64",
      "userId": "08371370297576666133"
     },
     "user_tz": -720
    },
    "id": "y0wGqp9Ec6pV",
    "outputId": "4fdf633c-81db-4f7f-aeff-d9c0ac9a8c55"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s12a6pPp740u"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "c63milaxDoCr"
   ],
   "name": "SubmissionNotebook.ipynb",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
